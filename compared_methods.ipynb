{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVR, SVC\n",
    "from scipy.sparse import vstack, hstack, coo_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msle(y_pred, y_true):\n",
    "    first_log = np.log(np.clip(y_pred, np.finfo(float).eps, None) + 1.)\n",
    "    second_log = np.log(y_true + 1.)\n",
    "    return np.mean(np.square(first_log - second_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51260"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### FORTWNW KATEUTHIAN TO WORDS_LIST GIA NA MIN KANW LEMMATIZE KATHE FORA 12 LEPTA!!!!!!\n",
    "\n",
    "with open('lemmatized_titles.pk', 'rb') as handle:\n",
    "    words_list = pickle.load(handle)\n",
    "len(words_list)  ###prepei na einai o arithmos twn arthrwn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"article_id.pk\", 'rb') as handle:\n",
    "    article_id = pickle.load(handle).tolist()\n",
    "len(article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_data_emb.pk', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "with open('training_set.pickle', 'rb') as handle:\n",
    "    train_set = pickle.load(handle)\n",
    "\n",
    "with open('test_set.pickle', 'rb') as handle:\n",
    "    test_set = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf = TfidfVectorizer(min_df=3, max_df=0.50, max_features=10000)\n",
    "text_sparse = text_tfidf.fit_transform([' '.join(x) for x in words_list]).astype(np.float32)\n",
    "np.save('tfidf_titles', text_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW TRAIN/TEST SETS\n",
    "import pandas as pd\n",
    "\n",
    "week = []\n",
    "l =['00','01','02','06','07','08']\n",
    "for i in l:\n",
    "    week.append(pd.read_csv('Access_Statistics/week'+i+'.txt', header = None, sep = '\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27941 4366\n"
     ]
    }
   ],
   "source": [
    "###to train tha einai ta unique twn 2 prwtwn bdomadwn kai to test ta unique tis 3is...episis lambanw upopsin to\n",
    "### impression mono tis prwti fora pou emfanizetai to arthro\n",
    "split_train1 = {}\n",
    "\n",
    "for w in week[:2]:    #2 prwtes bdomades(0,1)\n",
    "    for i,k in zip(w[1],w[2]):     # 1:id , 2:impression\n",
    "        if i not in split_train1:\n",
    "            split_train1[i] = k\n",
    "\n",
    "            \n",
    "split_test1 = {}\n",
    "\n",
    "for w in week[2:3]:    #3i bdomada (dld index=2)...sto indexing pairnei tin arxi kai afinei to telos eksw\n",
    "     for i,k in zip(w[1],w[2]):\n",
    "            if i not in split_test1 and i not in split_train1:\n",
    "                split_test1[i] = k    # an den uparxei to dimiourgoume arxika\n",
    "            \n",
    "\n",
    "print(len(split_train1), len(split_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sunartisi arxikopoisis\n",
    "def prepare_set_regression(set_dict):\n",
    "    WC, A, P, Y = [], [], [], []  #WC= word codes, A= annotations, P= producers, Y= ta ids me ta impressions\n",
    "\n",
    "    for key,value in set_dict.items():    #key=id value=impression  ,items() method is used to return the list with all dictionary keys with values\n",
    "        if key in data:\n",
    "            Y.append(value)         ##to value einai to impression kai to key einai to article id\n",
    "            WC.append(text_sparse[article_id.index(key)])\n",
    "            A.append(data[key]['annotations'])\n",
    "            temp = np.zeros((5))\n",
    "            temp[data[key]['publishers']] = 1\n",
    "            P.append(temp)\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    X = vstack(WC)\n",
    "    A = coo_matrix(A).tocsr()\n",
    "    P = coo_matrix(P).tocsr()\n",
    "    X = hstack([X, A, P]).tocsr()\n",
    "\n",
    "    print(Y.shape, X.shape, A.shape, P.shape)\n",
    "    return Y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_set_classification(set_dict):\n",
    "    WC, A, P, Y = [], [], [], []  #WC= word codes, A= annotations, P= producers, Y= ta ids me ta impressions\n",
    "\n",
    "    for key,value in set_dict.items():    #key=id value=impression  ,items() method is used to return the list with all dictionary keys with values\n",
    "        if key in data:\n",
    "            if value<10:\n",
    "                Y.append(0)         ##to value einai to impression kai to key einai to article id\n",
    "            elif value<100:\n",
    "                Y.append(1)\n",
    "            elif value<1000:\n",
    "                Y.append(2)\n",
    "            else:\n",
    "                Y.append(3)\n",
    "            WC.append(text_sparse[article_id.index(key)])\n",
    "            A.append(data[key]['annotations'])\n",
    "            temp = np.zeros((5))\n",
    "            temp[data[key]['publishers']] = 1\n",
    "            P.append(temp)\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    X = vstack(WC)\n",
    "    A = coo_matrix(A).tocsr()\n",
    "    P = coo_matrix(P).tocsr()\n",
    "    X = hstack([X, A, P]).tocsr()\n",
    "\n",
    "    print(Y.shape, X.shape, A.shape, P.shape)\n",
    "    return Y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31058,) (31058, 11003) (31058, 998) (31058, 5)\n",
      "(4454,) (4454, 11003) (4454, 998) (4454, 5)\n"
     ]
    }
   ],
   "source": [
    "Y_train, X_train = prepare_set_regression(split_train1)\n",
    "Y_test, X_test = prepare_set_regression(split_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27926,) (27926, 11003) (27926, 998) (27926, 5)\n",
      "(4365,) (4365, 11003) (4365, 998) (4365, 5)\n"
     ]
    }
   ],
   "source": [
    "Y_train, X_train = prepare_set_classification(split_train1)\n",
    "Y_test, X_test = prepare_set_classification(split_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3110,) (3110, 11003)\n"
     ]
    }
   ],
   "source": [
    "train_ids = np.where(Y_train>100)[0]\n",
    "test_ids = np.where(Y_test>100)[0]\n",
    "X_train = X_train[train_ids]\n",
    "Y_train = Y_train[train_ids]\n",
    "X_test = X_test[test_ids]\n",
    "Y_test = Y_test[test_ids]\n",
    "print(Y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.304624373185877"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = LinearRegression().fit(X_train, Y_train).predict(X_test)\n",
    "msle(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.50012823573697"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', gamma='scale', verbose=True)\n",
    "Y_pred = svr_rbf.fit(X_train, Y_train).predict(X_test)\n",
    "msle(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13920071845532106, 445.40000000000003)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = Y_test.shape[0]*0.1\n",
    "len(set(Y_test.argsort().tolist()[::-1][:int(size)]).intersection(set(Y_pred.argsort().tolist()[::-1][:int(size)]))) / size, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11631148949597167"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "size = int(len(Y_test)*0.1)\n",
    "print(size)\n",
    "# threshold = -Y_test.sort()[:size]\n",
    "threshold = np.min(np.sort(Y_test, axis=0)[::-1][:size])\n",
    "y_true = Y_test > threshold\n",
    "\n",
    "average_precision_score(y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47949599083619704"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "Y_pred = logreg.fit(X_train, Y_train).predict(X_test)\n",
    "np.mean(Y_pred == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3214203894616266"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_rbf = SVC(kernel='rbf', gamma='scale')\n",
    "Y_pred = svc_rbf.fit(X_train, Y_train).predict(X_test)\n",
    "np.mean(Y_pred == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4350515463917526"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "Y_pred = clf.fit(X_train, Y_train).predict(X_test)\n",
    "np.mean(Y_pred == Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
